##############################################################################

# THIS FILE IS ONLY MEANT TO BE EDITED IF YOU ARE SURE!

# PROGRAM FAILURE IS LIKELY TO OCCUR IF YOU ARE UNSURE OF THE
# CONSEQUENCES OF YOUR CHANGES.

# IF YOU HAVE CHANGED VALUES AND ARE ENCOUNTERING ERRORS, 
# STASH YOUR CHANGES ('git stash' in a git bash console in acodet directory)

##############################################################################
    

####################  AUDIO PROCESSING PARAMETERS  ###########################
#!!! CHANGING THESE PARAMETERS WILL RESULT IN MODEL FAILURE, ONLY TO      !!!#
#!!! BE CHANGED FOR TRAINING OF NEW MODEL ESPECIALY FOR DIFFERENT SPECIES !!!#
## Global audio processing parameters
# sample rate
sample_rate: 2000
# length of context window in seconds 
# this is the audio segment length that the model is trained on
context_window_in_seconds: 3.9

## Mel-Spectrogram parameters
# FFT window length
stft_frame_len: 1024
# number of time bins for mel spectrogram
number_of_time_bins: 128


###################  TFRECORD CREATION PARAMETERS  ###########################
## Settings for Creation of Tfrecord Dataset
# limit of context windows in a tfrecords file
tfrecs_limit_per_file: 600
# train/test split
train_ratio: 0.7
# test/val split
test_val_ratio: 0.7

########################  ANNOTATIONS ########################################
default_threshold: 0.5
# minimum frequency of annotation boxes
annotation_df_fmin: 50
# maximum frequency of annotation boxes
annotation_df_fmax: 1000
# scale the maximum frequency with the prediction value
# that way it is easier to validate in Raven
scale_fmax_with_predictions: True

########################  PATHS  #############################################
# dataset destination directory - save tfrecord dataset to this directory 
# only change when really necessary
tfrecords_destination_folder: '../Data/MyNewTFRecordsDataset'
# default folder to store newly created annotations
generated_annotations_folder: '../generated_annotations'
# source path for automatically generated combined annotations 
# Only relevant for creation of new dataset (option run_config: 2)
annotation_destination: '../combined_annotations'
# name of current North Atlantic humpback whale song model
model_name: 'Humpback_20221130'
# name of top level directory when annotating multiple datasets
top_dir_name: 'main'
# custom string to add to timestamp for directory name 
# of created annotations
annots_timestamp_folder: ''
# default threshold folder name
thresh_label: 'thresh_0.5'
# save downsmapled audio files for reprocessing?
save_downsampled_files: False

#######################  TRAINING  ###########################################

# Name of Model class, default is HumpBackNorthAtlantic, possible other classes
# are GoogleMod for the modified ResNet-50 architecture, or KerasAppModel for 
# any of the Keras application models (name will get specified under keras_mod_name)
ModelClassName: 'HumpBackNorthAtlantic' 
# ModelClassName: 'KerasAppModel' 
# batch size for training and evaluating purposes
batch_size: 32
# number of epochs to run the model
epochs: 150
# specify the path to your training checkpoint here to load a pretrained model
load_ckpt_path: False
# to run the google model, select True
load_g_ckpt: False
# specify the name of the keras application model that you want to run - select the
# ModelClassName KerasAppModel for this
keras_mod_name: False
# keras_mod_name: "EfficientNetB4"
# number of steps per epoch
steps_per_epoch: 400
# select True if you want your training data to be time shift augmented (recommended)
time_augs: False 
# select True if you want your training data to be MixUp augmented (recommended)
mixup_augs: False
# select True if you want your training data to be 
# time and frequency masked augmented (recommended)
spec_aug: False
# specify a string to describe the dataset used for this model run (to later be able
# to understand what was significant about this model training)
data_description: 'Large active learning from false positives from Ilaria. Complete shuffling of data'
# starting learning rate
init_lr: 5e-4
# final learning rate
final_lr: 5e-6
# number of preliminary blocks in model (are kept frozen)
pre_blocks: 9
# threshold for f score beta
f_score_beta: 0.5
f_score_thresh: 0.5
# number of layers to unfreeze, if False, the entire model is trainable
unfreeze: False

########################  Streamlit  #########################################
# select True if you want to run the streamlit app
streamlit: False
